{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CClDkW5KMH6x",
        "outputId": "6e1607c2-1d67-4d53-f529-f40b5ee3f3a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 324 candidates, totalling 972 fits\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries for visualization and data processing\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.compose import make_column_selector\n",
        "import numpy as np\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"/weather.csv\")  # Update with the correct dataset path\n",
        "\n",
        "# Data cleaning functions\n",
        "def data_preparation_0(df):\n",
        "    for i in df.columns:\n",
        "        l = i.split(\"_\")\n",
        "        if len(l) != 1:\n",
        "            if len(l) > 2:\n",
        "                i1 = l[1] + l[2]\n",
        "            else:\n",
        "                i1 = l[1]\n",
        "            i1 += \"_\" + l[0]\n",
        "            df.rename(columns={i: i1}, inplace=True)\n",
        "    return df\n",
        "\n",
        "def data_preparation_1(df):\n",
        "    df[\"id\"] = df.index\n",
        "    df_long = pd.wide_to_long(df.reset_index(), stubnames=[\n",
        "        'precipitation', 'tempmean', 'tempmin',\n",
        "        'tempmax', 'windspeed', 'pressure'], i=['DATE'], j='town', sep='_', suffix='.+')\n",
        "    df_long.reset_index()\n",
        "    df3 = df_long[['precipitation', 'tempmean', 'tempmin',\n",
        "                   'tempmax', 'windspeed', 'pressure']]\n",
        "    df3.reset_index(inplace=True)\n",
        "    return df3\n",
        "\n",
        "def extract_date_features(df):\n",
        "    df['DATE'] = pd.to_datetime(df['DATE'], format='%Y%m%d')\n",
        "    df['year'] = df['DATE'].dt.year\n",
        "    df['month'] = df['DATE'].dt.month\n",
        "    df['day'] = df['DATE'].dt.day\n",
        "    df = df.drop(columns='DATE')\n",
        "    return df\n",
        "\n",
        "def day_in_Life(df, number):\n",
        "    for i in range(1, number + 1):\n",
        "        df[[f\"tempmean{i}\", f\"tempmax{i}\"]] = df.groupby(['town'])[\n",
        "            [\"tempmean\", \"tempmax\"]].shift(i)\n",
        "    return df\n",
        "\n",
        "# Apply the data cleaning functions\n",
        "df = data_preparation_0(df)\n",
        "df = data_preparation_1(df)\n",
        "df = extract_date_features(df)\n",
        "df = day_in_Life(df, 2)\n",
        "\n",
        "# Define the preprocessing pipeline with scaling and PCA\n",
        "num_selector = make_column_selector(dtype_include=np.number)\n",
        "num_tree_processor = make_pipeline(\n",
        "    SimpleImputer(strategy=\"mean\", add_indicator=True),\n",
        "    StandardScaler(),  # Scaling step\n",
        "    PCA()\n",
        ")\n",
        "tree_preprocessor = make_column_transformer((num_tree_processor, num_selector))\n",
        "\n",
        "# Define the pipeline with the RandomForestRegressor\n",
        "pipe = Pipeline([\n",
        "    ('preprocessor', tree_preprocessor),\n",
        "    ('reg', RandomForestRegressor(random_state=42))\n",
        "])\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'preprocessor__pipeline__pca__n_components': [4, 6, 8],  # Test different PCA components\n",
        "    'reg__n_estimators': [50, 100, 150],                   # Number of trees in the forest\n",
        "    'reg__max_depth': [None, 10, 20, 30],                  # Maximum depth of the tree\n",
        "    'reg__min_samples_split': [2, 5, 10],                  # Minimum number of samples to split a node\n",
        "    'reg__min_samples_leaf': [1, 2, 4]                     # Minimum number of samples at a leaf node\n",
        "}\n",
        "\n",
        "# Set up the GridSearchCV with 3-fold cross-validation\n",
        "grid = GridSearchCV(pipe, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2, scoring='r2')\n",
        "\n",
        "# Prepare data for training\n",
        "x = df[['precipitation', 'windspeed', 'pressure', 'year', 'month', 'day']]\n",
        "y = df['tempmean']\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit the model using GridSearchCV\n",
        "grid.fit(x_train, y_train)\n",
        "\n",
        "# Print the best parameters and the best score from GridSearchCV\n",
        "print(\"Meilleur score R2:\", grid.best_score_)\n",
        "print(\"Meilleurs param√®tres:\", grid.best_params_)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "print(\"Score sur le jeu de test:\", grid.score(x_test, y_test))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
